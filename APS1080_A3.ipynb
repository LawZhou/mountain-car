{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "APS1080-A3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN/fF245R1IOFe8xQMygFPE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LawZhou/mountain-car/blob/main/APS1080_A3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQsMY_FCX8s6"
      },
      "source": [
        "import numpy as np\r\n",
        "import gym\r\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yw7CGx4shipV"
      },
      "source": [
        "class MountainCarBaseAgent():\r\n",
        "    def __init__(self, env, num_episodes, bin, min_lr, epsilon, lr,\r\n",
        "                 discount_factor, decay):\r\n",
        "        self.bin = bin\r\n",
        "        self.num_episodes = num_episodes\r\n",
        "        self.min_lr = min_lr\r\n",
        "        self.epsilon = epsilon\r\n",
        "        self.discount_factor = discount_factor\r\n",
        "        self.decay = decay\r\n",
        "        self.learning_rate = lr\r\n",
        "\r\n",
        "        self.env = env\r\n",
        "\r\n",
        "        self.upper_bounds = self.env.observation_space.high\r\n",
        "        self.lower_bounds = self.env.observation_space.low\r\n",
        "\r\n",
        "        self.position_bins = np.linspace(self.lower_bounds[0], self.upper_bounds[0], num=self.bin)\r\n",
        "        self.velocity_bins = np.linspace(self.lower_bounds[1], self.upper_bounds[1], num=self.bin)\r\n",
        "        self.Q = np.zeros((self.bin, self.bin, self.env.action_space.n))\r\n",
        "\r\n",
        "    def discretize_state(self, obs):\r\n",
        "        discrete_pos = np.digitize(obs[0], bins=self.position_bins)\r\n",
        "        discrete_vel = np.digitize(obs[1], bins=self.velocity_bins)\r\n",
        "        discrete_state = np.array([discrete_pos, discrete_vel]).astype(np.int)\r\n",
        "        return tuple(discrete_state)\r\n",
        "\r\n",
        "    def choose_action(self, state, greedy=False):\r\n",
        "        if not greedy:\r\n",
        "            if np.random.random() < self.epsilon:\r\n",
        "                return self.env.action_space.sample()\r\n",
        "            else:\r\n",
        "                return np.argmax(self.Q[state])\r\n",
        "        else:\r\n",
        "            return np.argmax(self.Q[state])\r\n",
        "\r\n",
        "\r\n",
        "    def get_learning_rate(self):\r\n",
        "        return max(self.min_lr, self.learning_rate - self.learning_rate*self.decay)\r\n",
        "\r\n",
        "    def run(self):\r\n",
        "        state = self.env.reset()\r\n",
        "        total_reward = 0\r\n",
        "        for ep in range(50000):\r\n",
        "            state = self.discretize_state(state)\r\n",
        "            action = self.choose_action(state, greedy=True)\r\n",
        "            obs, reward, done, info = self.env.step(action)\r\n",
        "            total_reward += reward\r\n",
        "            if done:\r\n",
        "                break\r\n",
        "            state = obs\r\n",
        "        return total_reward\r\n",
        "\r\n",
        "    def run_episodes(self, play_eps=500):\r\n",
        "        stepsRecorder = []\r\n",
        "        for _ in range(play_eps):\r\n",
        "            stepsRecorder.append(self.run())\r\n",
        "        stepsRecorder = np.array(stepsRecorder)\r\n",
        "        print(f'Finish with mean steps: {np.mean(stepsRecorder)} in {play_eps} episodes')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqKsTr1IhqPt"
      },
      "source": [
        "# Task 1\r\n",
        "Develop a TD(0) controller using:\r\n",
        "\r\n",
        "<ul>\r\n",
        "<li>on-policy SARSA</li>\r\n",
        "<li>on-policy expected SARSA</li>\r\n",
        "<li>off-policy expected SARSA with a greedy control policy.</li>\r\n",
        "</ul>\r\n",
        "\r\n",
        "Compare the performance of your controllers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSHrCxskhmJZ"
      },
      "source": [
        "class MountainCarTD0Agent(MountainCarBaseAgent):\r\n",
        "    def __init__(self, env, mode='off_expected_sarsa', bin=20, num_episodes=1000, min_lr=0.1, epsilon=0.2, lr=1.0,\r\n",
        "                 discount_factor=0.95, decay=0.25):\r\n",
        "        super().__init__(env, num_episodes, bin, min_lr, epsilon, lr,\r\n",
        "                 discount_factor, decay)\r\n",
        "        self.mode = mode\r\n",
        "\r\n",
        "    def train(self):\r\n",
        "        if self.mode == 'off_expected_sarsa':\r\n",
        "            self.train_off_expected_sarsa()\r\n",
        "        elif self.mode == 'sarsa':\r\n",
        "            self.train_sarsa()\r\n",
        "        else:\r\n",
        "            self.train_expected_sarsa()\r\n",
        "\r\n",
        "    def train_off_expected_sarsa(self):\r\n",
        "        for _ in tqdm(range(self.num_episodes)):\r\n",
        "            state = self.env.reset()\r\n",
        "            state = self.discretize_state(state)\r\n",
        "            self.learning_rate = self.get_learning_rate()\r\n",
        "            done = False\r\n",
        "            while not done:\r\n",
        "                action = self.choose_action(state)\r\n",
        "                next_state, reward, done, _ = self.env.step(action)\r\n",
        "                next_state = self.discretize_state(next_state)\r\n",
        "                self.update_off_expected_sarsa_Q(state, action, reward, next_state)\r\n",
        "                state = next_state\r\n",
        "\r\n",
        "    def train_sarsa(self):\r\n",
        "        for _ in tqdm(range(self.num_episodes)):\r\n",
        "            state = self.env.reset()\r\n",
        "            state = self.discretize_state(state)\r\n",
        "            self.learning_rate = self.get_learning_rate()\r\n",
        "            done = False\r\n",
        "            action = self.choose_action(state)\r\n",
        "            while not done:\r\n",
        "                next_state, reward, done, _ = self.env.step(action)\r\n",
        "                next_state = self.discretize_state(next_state)\r\n",
        "                next_action = self.choose_action(next_state)\r\n",
        "                self.update_sarsa_Q(state, action, reward, next_state, next_action)\r\n",
        "                state = next_state\r\n",
        "                action = next_action\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    def train_expected_sarsa(self):\r\n",
        "        for _ in tqdm(range(self.num_episodes)):\r\n",
        "            state = self.env.reset()\r\n",
        "            state = self.discretize_state(state)\r\n",
        "            self.learning_rate = self.get_learning_rate()\r\n",
        "            done = False\r\n",
        "            while not done:\r\n",
        "                action = self.choose_action(state)\r\n",
        "                next_state, reward, done, _ = self.env.step(action)\r\n",
        "                next_state = self.discretize_state(next_state)\r\n",
        "                self.update_expected_sarsa_Q(state, action, reward, next_state)\r\n",
        "                state = next_state\r\n",
        "\r\n",
        "    def update_sarsa_Q(self, state, action, reward, next_state, next_action):\r\n",
        "        self.Q[state][action] += self.learning_rate * (\r\n",
        "                reward + self.discount_factor * self.Q[next_state][next_action] - self.Q[state][action])\r\n",
        "\r\n",
        "    def update_off_expected_sarsa_Q(self, state, action, reward, next_state):\r\n",
        "        # greedy policy\r\n",
        "        self.Q[state][action] += self.learning_rate * (\r\n",
        "                reward + self.discount_factor * np.max(self.Q[next_state]) - self.Q[state][action])\r\n",
        "\r\n",
        "    def update_expected_sarsa_Q(self, state, action, reward, next_state):\r\n",
        "        At_prob = 1 - self.epsilon\r\n",
        "        random_action_prob = self.epsilon * (1 / self.env.action_space.n)\r\n",
        "        best_action = np.argmax(self.Q[next_state])\r\n",
        "        expected_sarsa = (At_prob+random_action_prob)*self.Q[next_state][best_action] # prob of best action * corresponding Q\r\n",
        "        for ac in range(self.env.action_space.n):\r\n",
        "            if ac != best_action:\r\n",
        "                expected_sarsa += random_action_prob*self.Q[next_state][ac] # prob of non-greedy action * corresponding Q\r\n",
        "\r\n",
        "        self.Q[state][action] += self.learning_rate * (\r\n",
        "                reward + self.discount_factor * expected_sarsa - self.Q[state][action])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8IfQYIfirDC"
      },
      "source": [
        "env = gym.make('MountainCar-v0')\r\n",
        "bin = 20\r\n",
        "num_episodes=5000\r\n",
        "min_lr=0.1\r\n",
        "epsilon=0.2\r\n",
        "lr=1.0\r\n",
        "discount_factor=0.95\r\n",
        "lr_decay=0.25"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-y8jSxdkhsmh",
        "outputId": "36e8c7a9-d9c4-4322-f9e9-c6e22184150d"
      },
      "source": [
        "print('train using off expected SARSA learning:')\r\n",
        "env.reset()\r\n",
        "agent = MountainCarTD0Agent(env, mode='off_expected_sarsa', \r\n",
        "                            num_episodes=num_episodes, \r\n",
        "                            min_lr=min_lr, epsilon=epsilon, \r\n",
        "                            lr=lr, discount_factor=discount_factor, \r\n",
        "                            decay=lr_decay)\r\n",
        "agent.train()\r\n",
        "agent.run_episodes()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 5/5000 [00:00<01:47, 46.67it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train using off expected SARSA learning:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [01:41<00:00, 49.18it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finish with mean steps: -167.186 in 500 episodes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ww2sbPTlhwYO",
        "outputId": "179d1d90-928f-4837-f418-ceca9d4dee37"
      },
      "source": [
        "print('train using SARSA: ')\r\n",
        "env.reset()\r\n",
        "agent = MountainCarTD0Agent(env, mode='sarsa',\r\n",
        "                            num_episodes=num_episodes, \r\n",
        "                            min_lr=min_lr, epsilon=epsilon, \r\n",
        "                            lr=lr, discount_factor=discount_factor, \r\n",
        "                            decay=lr_decay)\r\n",
        "agent.train()\r\n",
        "agent.run_episodes()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 5/5000 [00:00<01:40, 49.51it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train using SARSA: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [01:29<00:00, 56.03it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finish with mean steps: -155.772 in 500 episodes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtgb7vvkhwiA",
        "outputId": "70b5a42e-0fc8-4a97-ef8a-5106c91d5628"
      },
      "source": [
        "print('train using expected SARSA: ')\r\n",
        "env.reset()\r\n",
        "agent = MountainCarTD0Agent(env, mode='expected_sarsa',\r\n",
        "                            num_episodes=num_episodes, \r\n",
        "                            min_lr=min_lr, epsilon=epsilon, \r\n",
        "                            lr=lr, discount_factor=discount_factor, \r\n",
        "                            decay=lr_decay)\r\n",
        "agent.train()\r\n",
        "agent.run_episodes()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 4/5000 [00:00<02:13, 37.51it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train using expected SARSA: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [01:40<00:00, 49.98it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finish with mean steps: -135.36 in 500 episodes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWZlPxR7KOCV"
      },
      "source": [
        "|                           | Score  |\r\n",
        "|---------------------------|--------|\r\n",
        "| SARSA                     | -155.8 |\r\n",
        "| on-policy expected SARSA  | -135.4 |\r\n",
        "| off-policy expected SARSA | -167.2 |\r\n",
        "\r\n",
        "On-policy expected SARSA achieves a higher score than SARSA which is expected in general as it eliminates the variance introduces by the random action in SARSA. Off-policy expected SARSA behaves exactly the same with Q-learning and it gets the worst result. On-policy often suffers from the diversity of the policy, resulting not enough sampling of Q. It also might be due to the choice of hyperparameters as well. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXblkvgahz_7"
      },
      "source": [
        "# Task 2\r\n",
        "Code controllers for TD(2), TD(3), and TD(4) using n-SARSA. Assess performance and compare against TD(0) and each other."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqQiL2b1hzWD"
      },
      "source": [
        "class MountainCarTDnAgent(MountainCarBaseAgent):\r\n",
        "    def __init__(self, env, n, bin=20, num_episodes=1000, discount_factor=0.95, min_lr=0.1, lr=1.0,\r\n",
        "                 decay=0.25, epsilon=0.2):\r\n",
        "        super().__init__(env, num_episodes, bin, min_lr, epsilon, lr,\r\n",
        "                         discount_factor, decay)\r\n",
        "        self.n = n\r\n",
        "        self.state_store = {}\r\n",
        "        self.action_store = {}\r\n",
        "        self.reward_store = {}\r\n",
        "\r\n",
        "    def train(self):\r\n",
        "\r\n",
        "        for ep in tqdm(range(self.num_episodes)):\r\n",
        "            T = np.inf\r\n",
        "            tau = 0\r\n",
        "            t = -1\r\n",
        "\r\n",
        "            state = self.env.reset()\r\n",
        "\r\n",
        "            state = self.discretize_state(state)\r\n",
        "            action = self.choose_action(state)\r\n",
        "            self.state_store[0] = state\r\n",
        "            self.action_store[0] = action\r\n",
        "\r\n",
        "            self.learning_rate = self.get_learning_rate()\r\n",
        "\r\n",
        "            while tau < (T - 1):\r\n",
        "                t += 1\r\n",
        "                if t < T:\r\n",
        "                    state, reward, done, _ = self.env.step(action)\r\n",
        "                    state = self.discretize_state(state)\r\n",
        "                    self.state_store[(t+1) % (self.n+1)] = state\r\n",
        "                    self.reward_store[(t+1) % (self.n+1)] = reward\r\n",
        "\r\n",
        "\r\n",
        "                    if done:\r\n",
        "                        T = t + 1\r\n",
        "                    else:\r\n",
        "                        action = self.choose_action(state)\r\n",
        "                        self.action_store[(t+1) % (self.n+1)] = action\r\n",
        "                tau = t - self.n + 1\r\n",
        "\r\n",
        "                if tau >= 0:\r\n",
        "                    G = np.sum([self.discount_factor**(i-tau-1) * self.reward_store[i % (self.n+1)] for i in range(tau+1, min(tau+self.n, T)+1)])\r\n",
        "                    if tau + self.n < T:\r\n",
        "                        state_tau = self.state_store[(tau + self.n) % (self.n+1)]\r\n",
        "                        action_tau = self.action_store[(tau + self.n) % (self.n+1)]\r\n",
        "                        G += (self.discount_factor ** self.n) * self.Q[state_tau][action_tau]\r\n",
        "                    state_tau, action_tau = self.state_store[tau % (self.n+1)], self.action_store[tau % (self.n+1)]\r\n",
        "                    self.Q[state_tau][action_tau] += self.learning_rate * (G-self.Q[state_tau][action_tau])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvzHUSw1h6Yg"
      },
      "source": [
        "env = gym.make('MountainCar-v0')\r\n",
        "bin = 20\r\n",
        "num_episodes=5000\r\n",
        "min_lr=0.1\r\n",
        "epsilon=0.2\r\n",
        "lr=1.0\r\n",
        "discount_factor=0.95\r\n",
        "lr_decay=0.25"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muwb-hOhimkU",
        "outputId": "fd28bb40-c775-4fcf-fe15-f66345f41fc0"
      },
      "source": [
        "print('train using n=2: ')\r\n",
        "env.reset()\r\n",
        "agent = MountainCarTDnAgent(env, n=2,\r\n",
        "                            num_episodes=num_episodes, \r\n",
        "                            min_lr=min_lr, epsilon=epsilon, \r\n",
        "                            lr=lr, discount_factor=discount_factor, \r\n",
        "                            decay=lr_decay)\r\n",
        "agent.train()\r\n",
        "agent.run_episodes()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 5/5000 [00:00<01:52, 44.51it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train using n=2: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [01:53<00:00, 44.03it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finish with mean steps: -155.938 in 500 episodes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrD4Pdynimm-",
        "outputId": "90ed5222-8e26-4ff8-9a6c-8542caeb18a7"
      },
      "source": [
        "print('train using n=3: ')\r\n",
        "env.reset()\r\n",
        "agent = MountainCarTDnAgent(env, n=3,\r\n",
        "                            num_episodes=num_episodes, \r\n",
        "                            min_lr=min_lr, epsilon=epsilon, \r\n",
        "                            lr=lr, discount_factor=discount_factor, \r\n",
        "                            decay=lr_decay)\r\n",
        "agent.train()\r\n",
        "agent.run_episodes()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 5/5000 [00:00<01:53, 43.99it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train using n=3: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [01:48<00:00, 46.05it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finish with mean steps: -162.28 in 500 episodes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJKe3B-kioAU",
        "outputId": "406baf01-75f5-4e50-b1d4-a95084777a8c"
      },
      "source": [
        "print('train using n=4: ')\r\n",
        "env.reset()\r\n",
        "agent = MountainCarTDnAgent(env, n=4,\r\n",
        "                            num_episodes=num_episodes, \r\n",
        "                            min_lr=min_lr, epsilon=epsilon, \r\n",
        "                            lr=lr, discount_factor=discount_factor, \r\n",
        "                            decay=lr_decay)\r\n",
        "agent.train()\r\n",
        "agent.run_episodes()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 5/5000 [00:00<01:54, 43.58it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train using n=4: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5000/5000 [01:50<00:00, 45.14it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finish with mean steps: -156.316 in 500 episodes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGjqyF8tNy1h"
      },
      "source": [
        "|                           | Score  |\r\n",
        "|---------------------------|--------|\r\n",
        "| TD(2)                     | -155.9 |\r\n",
        "|  TD(2)                    | -162.3 |\r\n",
        "|  TD(4)                    | -156.3 |\r\n",
        "\r\n",
        "Overall, the performance between the three n-step SARSA doesn't vary very much. Compared to the performance of TD(0), n-step SARSA achieves similar results with SARSA but off-policy expected SARSA still outperforms other methods.  "
      ]
    }
  ]
}